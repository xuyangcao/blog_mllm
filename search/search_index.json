{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u524d\u8a00 \u8fd9\u4e0d\u662f\u4e00\u672c\"\u6559\u7a0b\u578b\"\u7684\u4e66\uff0c\u800c\u662f\u4e00\u672c \u9762\u5411\u5de5\u7a0b\u5b9e\u8df5\u4e0e\u7814\u7a76\u601d\u8003\u7684\u6280\u672f\u7b14\u8bb0\u96c6 \u3002 \u672c\u4e66\u76ee\u6807 \u5f88\u591a\u6280\u672f\u6587\u7ae0\u8981\u4e48\u53ea\u8bb2\u516c\u5f0f\u4e0d\u8c08\u5b9e\u73b0\uff0c\u8981\u4e48\u53ea\u8d34\u4ee3\u7801\u4e0d\u89e3\u91ca\u539f\u7406\u3002\u8fd9\u672c\u4e66\u8bd5\u56fe\u67b6\u8d77\u4e24\u8005\u4e4b\u95f4\u7684\u6865\u6881\uff1a \u516c\u5f0f\u62c6\u89e3 \uff1a\u4e0d\u53ea\u5217\u51fa\u516c\u5f0f\uff0c\u800c\u662f\u9010\u5c42\u62c6\u89e3\uff0c\u89e3\u91ca\u6bcf\u4e2a\u7b26\u53f7\u7684\u542b\u4e49\u548c\u8bbe\u8ba1\u610f\u56fe \u7c7b\u6bd4\u8f85\u52a9 \uff1a\u7528\u719f\u6089\u7684\u6982\u5ff5\uff08\u5982\u76d1\u7763\u5b66\u4e60\uff09\u7c7b\u6bd4\u964c\u751f\u7684\u9886\u57df\uff08\u5982\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u964d\u4f4e\u7406\u89e3\u95e8\u69db \u5de5\u7a0b\u89c6\u89d2 \uff1a\u89e3\u91ca\"\u4e3a\u4ec0\u4e48\u8fd9\u6837\u8bbe\u8ba1\"\u2014\u2014\u6bd4\u5982\u4e3a\u4ec0\u4e48\u8981\u505a\u5f52\u4e00\u5316\u3001\u4e3a\u4ec0\u4e48\u8981 clip\u3001\u5982\u4f55\u9632\u6b62\u68af\u5ea6\u7206\u70b8 \u4ee3\u7801\u5bf9\u7167 \uff1a\u63d0\u4f9b\u4f2a\u4ee3\u7801\u6216\u6838\u5fc3\u5b9e\u73b0\uff0c\u5e2e\u52a9\u5c06\u6570\u5b66\u516c\u5f0f\u6620\u5c04\u5230\u5b9e\u9645\u4ee3\u7801 \u9002\u5408\u8c01\u8bfb \u5982\u679c\u4f60\u6b63\u5728\u505a\u4ee5\u4e0b\u65b9\u5411\u7684\u7814\u7a76\u6216\u5de5\u7a0b\u5de5\u4f5c\uff1a \ud83d\udd25 \u591a\u6a21\u6001\u5927\u6a21\u578b\uff08VLM\u3001MLLM\uff09 \ud83e\udde0 \u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e0e\u4f18\u5316 \u26a1 \u5206\u5e03\u5f0f\u8bad\u7ec3\u4e0e\u63a8\u7406\u52a0\u901f \ud83c\udfaf \u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\uff08RLHF\u3001GRPO\u3001PPO\uff09 \u5e0c\u671b\u8fd9\u672c\u4e66\u80fd\u5e2e\u4f60\u5c11\u8d70\u5f2f\u8def\uff0c\u628a\u539f\u7406\u641e\u900f\u3002 \ud83d\udcda \u5185\u5bb9\u76ee\u5f55 \u5206\u5e03\u5f0f\u8bad\u7ec3 \u7ae0\u8282 \u72b6\u6001 \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 \ud83d\udcdd TODO \u68af\u5ea6\u7d2f\u79ef \ud83d\udcdd TODO \u6d41\u6c34\u7ebf\u5e76\u884c \ud83d\udcdd TODO \u6570\u636e\u5e76\u884c \ud83d\udcdd TODO \u6a21\u578b\u5e76\u884c \ud83d\udcdd TODO \u5e76\u884c\u8bad\u7ec3\u4f18\u5316 \ud83d\udcdd TODO Transformer \u7ae0\u8282 \u72b6\u6001 Attention \u673a\u5236 \ud83d\udcdd TODO KV Cache \u539f\u7406 \ud83d\udcdd TODO RoPE \u539f\u7406 \ud83d\udcdd TODO \u4f4d\u7f6e\u7f16\u7801 \ud83d\udcdd TODO \u81ea\u6ce8\u610f\u529b\u673a\u5236 \ud83d\udcdd TODO \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 \ud83d\udcdd TODO \u5f3a\u5316\u5b66\u4e60 \u7ae0\u8282 \u72b6\u6001 PPO \u539f\u7406 \ud83d\udcdd TODO GRPO \u539f\u7406 \u2705 \u5df2\u5b8c\u6210 \u6a21\u578b\u89e3\u6790 \u7ae0\u8282 \u72b6\u6001 Qwen-VL\u7cfb\u5217\u6a21\u578b\u89e3\u6790 \ud83d\udcdd TODO InterVL\u7cfb\u5217\u6a21\u578b\u89e3\u6790 \ud83d\udcdd TODO DeepSeek\u7cfb\u5217\u6a21\u578b\u89e3\u6790 \ud83d\udcdd TODO","title":"\u524d\u8a00"},{"location":"#_1","text":"\u8fd9\u4e0d\u662f\u4e00\u672c\"\u6559\u7a0b\u578b\"\u7684\u4e66\uff0c\u800c\u662f\u4e00\u672c \u9762\u5411\u5de5\u7a0b\u5b9e\u8df5\u4e0e\u7814\u7a76\u601d\u8003\u7684\u6280\u672f\u7b14\u8bb0\u96c6 \u3002","title":"\u524d\u8a00"},{"location":"#_2","text":"\u5f88\u591a\u6280\u672f\u6587\u7ae0\u8981\u4e48\u53ea\u8bb2\u516c\u5f0f\u4e0d\u8c08\u5b9e\u73b0\uff0c\u8981\u4e48\u53ea\u8d34\u4ee3\u7801\u4e0d\u89e3\u91ca\u539f\u7406\u3002\u8fd9\u672c\u4e66\u8bd5\u56fe\u67b6\u8d77\u4e24\u8005\u4e4b\u95f4\u7684\u6865\u6881\uff1a \u516c\u5f0f\u62c6\u89e3 \uff1a\u4e0d\u53ea\u5217\u51fa\u516c\u5f0f\uff0c\u800c\u662f\u9010\u5c42\u62c6\u89e3\uff0c\u89e3\u91ca\u6bcf\u4e2a\u7b26\u53f7\u7684\u542b\u4e49\u548c\u8bbe\u8ba1\u610f\u56fe \u7c7b\u6bd4\u8f85\u52a9 \uff1a\u7528\u719f\u6089\u7684\u6982\u5ff5\uff08\u5982\u76d1\u7763\u5b66\u4e60\uff09\u7c7b\u6bd4\u964c\u751f\u7684\u9886\u57df\uff08\u5982\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u964d\u4f4e\u7406\u89e3\u95e8\u69db \u5de5\u7a0b\u89c6\u89d2 \uff1a\u89e3\u91ca\"\u4e3a\u4ec0\u4e48\u8fd9\u6837\u8bbe\u8ba1\"\u2014\u2014\u6bd4\u5982\u4e3a\u4ec0\u4e48\u8981\u505a\u5f52\u4e00\u5316\u3001\u4e3a\u4ec0\u4e48\u8981 clip\u3001\u5982\u4f55\u9632\u6b62\u68af\u5ea6\u7206\u70b8 \u4ee3\u7801\u5bf9\u7167 \uff1a\u63d0\u4f9b\u4f2a\u4ee3\u7801\u6216\u6838\u5fc3\u5b9e\u73b0\uff0c\u5e2e\u52a9\u5c06\u6570\u5b66\u516c\u5f0f\u6620\u5c04\u5230\u5b9e\u9645\u4ee3\u7801","title":"\u672c\u4e66\u76ee\u6807"},{"location":"#_3","text":"\u5982\u679c\u4f60\u6b63\u5728\u505a\u4ee5\u4e0b\u65b9\u5411\u7684\u7814\u7a76\u6216\u5de5\u7a0b\u5de5\u4f5c\uff1a \ud83d\udd25 \u591a\u6a21\u6001\u5927\u6a21\u578b\uff08VLM\u3001MLLM\uff09 \ud83e\udde0 \u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e0e\u4f18\u5316 \u26a1 \u5206\u5e03\u5f0f\u8bad\u7ec3\u4e0e\u63a8\u7406\u52a0\u901f \ud83c\udfaf \u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\uff08RLHF\u3001GRPO\u3001PPO\uff09 \u5e0c\u671b\u8fd9\u672c\u4e66\u80fd\u5e2e\u4f60\u5c11\u8d70\u5f2f\u8def\uff0c\u628a\u539f\u7406\u641e\u900f\u3002","title":"\u9002\u5408\u8c01\u8bfb"},{"location":"#_4","text":"","title":"\ud83d\udcda \u5185\u5bb9\u76ee\u5f55"},{"location":"#_5","text":"\u7ae0\u8282 \u72b6\u6001 \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 \ud83d\udcdd TODO \u68af\u5ea6\u7d2f\u79ef \ud83d\udcdd TODO \u6d41\u6c34\u7ebf\u5e76\u884c \ud83d\udcdd TODO \u6570\u636e\u5e76\u884c \ud83d\udcdd TODO \u6a21\u578b\u5e76\u884c \ud83d\udcdd TODO \u5e76\u884c\u8bad\u7ec3\u4f18\u5316 \ud83d\udcdd TODO","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3"},{"location":"#transformer","text":"\u7ae0\u8282 \u72b6\u6001 Attention \u673a\u5236 \ud83d\udcdd TODO KV Cache \u539f\u7406 \ud83d\udcdd TODO RoPE \u539f\u7406 \ud83d\udcdd TODO \u4f4d\u7f6e\u7f16\u7801 \ud83d\udcdd TODO \u81ea\u6ce8\u610f\u529b\u673a\u5236 \ud83d\udcdd TODO \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 \ud83d\udcdd TODO","title":"Transformer"},{"location":"#_6","text":"\u7ae0\u8282 \u72b6\u6001 PPO \u539f\u7406 \ud83d\udcdd TODO GRPO \u539f\u7406 \u2705 \u5df2\u5b8c\u6210","title":"\u5f3a\u5316\u5b66\u4e60"},{"location":"#_7","text":"\u7ae0\u8282 \u72b6\u6001 Qwen-VL\u7cfb\u5217\u6a21\u578b\u89e3\u6790 \ud83d\udcdd TODO InterVL\u7cfb\u5217\u6a21\u578b\u89e3\u6790 \ud83d\udcdd TODO DeepSeek\u7cfb\u5217\u6a21\u578b\u89e3\u6790 \ud83d\udcdd TODO","title":"\u6a21\u578b\u89e3\u6790"},{"location":"RL/grpo/","text":"GRPO\uff08Group Relative Policy Optimization\uff09\u7406\u89e3 GRPO\u7684\u6838\u5fc3\u601d\u60f3\u53ef\u4ee5\u7b80\u8981\u7406\u89e3\u4e3aGrouped PPO\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u5bf9\u540c\u4e00\u4e2a\u95ee\u9898 \\(q\\) , \u4e00\u6b21\u6027\u91c7\u6837\u4e00\u7ec4\uff08 \\(G\\) \u4e2a\uff09\u5b8c\u6574\u56de\u7b54\uff0c\u7528\u7ec4\u5185\u76f8\u5bf9\u4f18\u52bf\uff08\u800c\u4e0d\u662f value baseline\uff09\u6765\u505a PPO-style \u7684 token \u7ea7\u7b56\u7565\u66f4\u65b0\uff0c\u540c\u65f6\u7528KL\u6b63\u5219\u628a\u7b56\u7565\u7ea6\u675f\u5728\u53c2\u8003\u6a21\u578b\u9644\u8fd1\u3002 \u8fd9\u91cc\u7684Group\u6307\u7684\u5c31\u662f\u5206\u7ec4\uff1bRelative\u6307\u7684\u5c31\u662f\u7ec4\u5185\u76f8\u5bf9\u4f18\u52bf\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u60f3\u6839\u636eGRPO\u4e2d\u6d89\u53ca\u5230\u7684\u516c\u5f0f\uff0c\u548c\u5bf9\u5e94\u7684\u4f2a\u4ee3\u7801\uff0c\u6df1\u5165\u7406\u89e3\u4e00\u4e0bGRPO\u91cc\u9762\u7684\u7ec6\u8282\uff1b\u5982\u679c\u4f60\u5bf9\u516c\u5f0f\u4e0d\u611f\u5174\u8da3\uff0c\u90a3\u4e48\u53ef\u4ee5\u76f4\u63a5\u8df3\u8fc7\u516c\u5f0f\u56de\u987e\u548c\u6df1\u5165\u7406\u89e3\u7684\u90e8\u5206\u3002 \u516c\u5f0f\u56de\u987e \u539f\u6587\u4e2d\u516c\u5f0f\u5982\u4e0b\uff1a \\[ \\begin{aligned} \\mathcal{J}_{G R P O}(\\theta) & =\\mathbb{E}_{\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right]} \\\\ & \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|}\\left\\{\\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)} \\hat{A}_{i, t}, \\operatorname{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]-\\beta \\mathbb{D}_{K L}\\left[\\pi_\\theta| | \\pi_{r e f}\\right]\\right\\} \\end{aligned} % \\tag{1} \\] \u6211\u611f\u89c9\u4f5c\u8005\u7701\u7565\u4e86\u4e00\u5bf9\u62ec\u53f7\uff0c\u5bf9\u6570\u5b66\u4e0d\u597d\u7684\u540c\u5b66\u4e0d\u592a\u53cb\u597d\uff0c\u56e0\u6b64\u8fd9\u91cc\u7ed9\u5b83\u52a0\u4e0a\uff0c\u957f\u6210\u8fd9\u6837\uff1a \\[ \\begin{aligned} \\mathcal{J}_{\\text{GRPO}}(\\theta) &= \\mathbb{E}_{\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(o|q)\\right]} \\Bigg[ \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\Bigg\\{ \\min \\Bigg[ \\frac{\\pi_\\theta(o_{i,t}\\mid q, o_{i,<t})} {\\pi_{\\theta_{\\text{old}}}(o_{i,t}\\mid q, o_{i,<t})} \\hat{A}_{i,t}, \\\\ &\\quad\\quad \\operatorname{clip}\\!\\left( \\frac{\\pi_\\theta(o_{i,t}\\mid q, o_{i,<t})} {\\pi_{\\theta_{\\text{old}}}(o_{i,t}\\mid q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\Bigg] - \\beta \\, D_{\\text{KL}}\\!\\left[\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\right] \\Bigg\\} \\Bigg] \\end{aligned} \\] \u6574\u4e2a GRPO \u76ee\u6807\u51fd\u6570 \\(\\mathcal{J}_{\\text{GRPO}}(\\theta)\\) \u662f\u4e00\u4e2a\u671f\u671b\u6536\u76ca\uff08expected objective\uff09\uff0c\u5373\u6a21\u578b\u7b56\u7565 \\(\\pi_\\theta\\) \u5728\u5927\u91cf\u4e0d\u540c\u4efb\u52a1\uff08prompts\uff09\u548c\u751f\u6210\u7ed3\u679c(responses)\u4e0a\u7684\u6574\u4f53\u8868\u73b0\u6c34\u5e73\uff08\u7531 group-normalized advantage \u52a0\u6743\uff0c\u5e76\u51cf\u53bb KL \u6b63\u5219\u9879\u8868\u793a\uff09\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\uff1a\u901a\u8fc7\u8c03\u6574 \\(\\theta\\) \uff0c\u8ba9\u8fd9\u4e2a\u671f\u671b\u6536\u76ca\u5c3d\u53ef\u80fd\u5927\u3002 \u6ce8\uff1a\u5728\u5b9e\u9645\u8bad\u7ec3\u4e2d\uff0c\u4e3a\u4e86\u517c\u5bb9\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u5e38\u505a\u6cd5\u662f\u6700\u5c0f\u5316\u5176\u8d1f\u503c\uff0c\u5373\u5b9a\u4e49\uff1a $$ \\mathcal{L} {\\text{GRPO}}(\\theta) = -\\mathcal{J} {\\text{GRPO}}(\\theta) $$ \u5e76\u5bf9 \\(\\mathcal{L}_{\\text{GRPO}}(\\theta)\\) \u6267\u884c\u68af\u5ea6\u4e0b\u964d\u3002 \u6df1\u5165\u7406\u89e3 1. \u6700\u5916\u5c42\u671f\u671b\uff08Expectation\uff09\uff1a\u5728\u4ec0\u4e48\u5206\u5e03\u4e0a\u8fdb\u884c\u4f18\u5316\uff1f \\[ \\mathbb{E}_{[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(o|q)]} \\] \u5176\u4e2d, \\(q\\) \u8868\u793a\u4e00\u4e2a prompt\uff08\u95ee\u9898\uff09\uff0c\u4ece\u6570\u636e\u5206\u5e03 \\(P(Q)\\) \u4e2d\u91c7\u6837\uff0c \\(P(Q)\\) \u5305\u542b\u4e86\u4e0d\u540c\u7c7b\u578b\u7684prompt\uff0c\u4f8b\u5982\u533b\u5b66\u95ee\u9898\uff0cVQA\u3001\u62a5\u544a\u751f\u6210\u7b49\uff1b \\(\\{o_i\\}_{i=1}^G\\) \u8868\u793a\u5bf9\u540c\u4e00\u4e2a \\(q\\) \uff0c\u7528\u65e7\u7b56\u7565 \\(\\pi_{\\theta_{\\text{old}}}\\) \u751f\u6210\u7684 G \u4e2a\u8f93\u51fa\uff08completions\uff09 . \u4e5f\u5c31\u662f\u8bf4\uff1a \u5bf9\u6bcf\u4e2a prompt\uff0c\u6211\u4eec\u5e76\u884c\u751f\u6210 G \u4e2a\u56de\u7b54\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u56de\u7b54\u505a\u4f18\u5316 . \u8fd9\u5c31\u662f GRPO \u7684\u201c\u7ec4\u5185\u5bf9\u6bd4\u201d\u601d\u60f3. \u8fdb\u4e00\u6b65\u7406\u89e3\uff0c\u8fd9\u4e2a\u671f\u671b\u53ef\u4ee5\u8868\u793a\uff1a\"\u5982\u679c\u6211\u4eec\u4ece\u771f\u5b9e prompt \u5206\u5e03\u4e2d\u968f\u673a\u6311\u4e00\u4e2a \\(q\\) \uff0c\u518d\u7528\u65e7\u7b56\u7565\u5bf9\u5b83\u751f\u6210 \\(G\\) \u4e2a\u968f\u673a\u56de\u7b54\uff0c\u7136\u540e\u8ba1\u7b97 GRPO loss\uff0c\u90a3\u4e48\u8fd9\u4e2a loss \u7684\u5e73\u5747\u503c\u662f\u591a\u5c11?\"\uff0c\u800c\u6211\u4eec\u7684\u76ee\u6807\u662f\uff1a\u901a\u8fc7\u8c03\u6574 \\(\\theta\\) \uff08\u5f53\u524d\u7b56\u7565\uff09\uff0c\u8ba9\u8fd9\u4e2a\u671f\u671b loss \u5c3d\u53ef\u80fd\u5c0f\u3002\" \u8fd9\u91cc\u6211\u4eec\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u6765\u505a\u4e2a\u7c7b\u6bd4\uff0c\u76d1\u7763\u5b66\u4e60\u7684\u76ee\u6807\u51fd\u6570\u5982\u4e0b\uff1a \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} \\left[ \\text{CrossEntropy}(f_\\theta(x), y) \\right] \\] \u5176\u4e2d \\(\\mathcal{D}\\) \u8868\u793a\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c \\((x, y)\\) \u8868\u793a\u4e00\u6761\u6837\u672c\uff08\u5982\u56fe\u50cf+\u6807\u7b7e\uff09\uff0cCrossEntropy\u8868\u793a\u5bf9\u8fd9\u6761\u6837\u672c\u8ba1\u7b97\u7684 loss\u3002\u90a3\u4e48\u8fd9\u4e2a\u516c\u5f0f\u7684\u6574\u4f53\u542b\u4e49\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\u5728\u6240\u6709\u53ef\u80fd\u7684\u6570\u636e\u4e0a\uff0c\u6a21\u578b\u9884\u6d4b\u7684\u5e73\u5747\u4ea4\u53c9\u71b5\u635f\u5931\u3002 \u4f46\u6211\u4eec\u65e0\u6cd5\u8ba1\u7b97\u771f\u5b9e\u671f\u671b\uff08\u56e0\u4e3a \\(\\mathcal{D}\\) \u672a\u77e5\uff09\uff0c\u6240\u4ee5\u7528 Monte Carlo \u4f30\u8ba1 \uff08\u5373 mini-batch\uff09\uff1a \\[ \\hat{\\mathcal{L}}(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\text{CrossEntropy}(f_\\theta(x_i), y_i) \\] \u4e5f\u5c31\u662f\u8bf4\uff0c \u671f\u671b = \u7406\u8bba\u76ee\u6807\uff0c\u6c42\u548c = \u5b9e\u9645\u8fd1\u4f3c \u3002\u56de\u5230 GRPO,\u5b8c\u5168\u4e00\u6837\u7684\u903b\u8f91,\u5bf9\u5e94\u5173\u7cfb\u5982\u4e0b\uff1a \u76d1\u7763\u5b66\u4e60 GRPO \u6570\u636e\u5206\u5e03 \\(\\mathcal{D}\\) Prompt \u5206\u5e03 \\(P(Q)\\) + \u65e7\u7b56\u7565\u751f\u6210\u5206\u5e03 \\(\\pi_{\\theta_{\\text{old}}}\\) \u4e00\u6761\u6837\u672c \\((x, y)\\) \u4e00\u7ec4\u6570\u636e \\((q, o_1, ..., o_G)\\) Loss \u51fd\u6570\uff08\u5982 CE\uff09 PPO-style clipped objective + KL penalty \u671f\u671b \\(\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\text{loss}]\\) \\(\\mathbb{E}_{q, \\{o_i\\}}[\\text{GRPO loss}]\\) 2. \u5206\u7ec4\u5e73\u5747 && Token\u5e73\u5747\uff1a\u4fdd\u6301\u68af\u5ea6\u7a33\u5b9a\u7684\u540c\u65f6\u62c9\u9f50\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6\u7684\u8d21\u732e \u4ece\u4e0b\u9762\u7684\u516c\u5f0f\uff0c\u53ef\u4ee5\u770b\u5230\u8fd9\u91cc\u6709\u4e24\u5c42\u5e73\u5747\uff0c\u5916\u5c42\u5e73\u5747\u8868\u793a\u5bf9\u4e00\u4e2aprompt\u4ea7\u751f\u7684\u5206\u7ec4\u5185\u90e8loss\u505a\u5e73\u5747\uff0c\u907f\u514d \\(G\\) \u53d8\u5927\u5bfc\u81f4\u68af\u5ea6\u7206\u70b8\uff1b\u5185\u5c42\u5bf9token\u518d\u5e73\u5747\uff0c\u4f7f\u5f97\u4e0d\u540c\u957f\u5ea6\u7684\u56de\u7b54\u88ab\u5f52\u4e00\u5316\u5230\u76f8\u540c\u7684\u5c3a\u5ea6\uff0c\u9632\u6b62\u6a21\u578b\u751f\u6210\u66f4\u957f\u7684token\u6765\u5237reward\u3002 \\[ \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left\\{ \\cdots \\right\\} \\] \u5177\u4f53\u5730\uff0c\u5bf9\u4e8e\u6bcf\u4e2a output \\(o_i\\) \uff1a\u8ba1\u7b97\u5176\u6bcf\u4e2a token \\(t\\) \u5bf9\u5e94\u7684 loss\uff0c\u5176\u4e2d \\(|o_i|\\) \u8868\u793a \\(o_i\\) \u7684\u5e8f\u5217\u957f\u5ea6\uff1b \u7136\u540e\u5bf9\u6240\u6709 token \u53d6\u5e73\u5747\uff08 \\(\\frac{1}{|o_i|}\\) \uff09\uff1b \u518d\u5bf9\u6240\u6709 G \u4e2a output \u53d6\u5e73\u5747\uff08 \\(\\frac{1}{G}\\) \uff09\u3002 3. PPO-style Clipped Objective \u63a5\u4e0b\u6765\u6211\u4eec\u770b\u4e00\u4e0b\u5927\u62ec\u53f7\u5185\u90e8\uff0c\u5b9e\u9645\u4e0a\u662f\u65b0\u65e7\u7b56\u7565\u4e4b\u95f4\u7684token\u7ea7\u91cd\u8981\u6027\u91c7\u6837\u6bd4(Importance Sampling Ration, ISR)\u548c\u4f18\u52bf\u503c(Advantage)\u7684\u4e58\u79ef\uff0c\u5177\u4f53\u5730\uff1a \\[ \\min \\left[ \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\, \\text{clip}\\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\right] \\] \u5176\u4e2d\uff0c \\(\\pi_{\\theta}\\) \u8868\u793a\u5f53\u524d\u5f85\u4f18\u5316\u7684\u7b56\u7565\u6a21\u578b\uff08policy\uff09\uff0c \\(\\frac{\\pi_\\theta}{\\pi_{\\text{old}}}\\) \u4e3aISR\uff0c \\(o_{i,t}\\) \u8868\u793a\u7b2c \\(i\\) \u4e2a\u56de\u7b54\u7684\u7b2c \\(t\\) \u4e2a token\uff0c \\(o_{i,<t}\\) \u8868\u793a\u7b2c \\(i\\) \u4e2a\u56de\u7b54\u4e2d\u7b2c \\(t\\) \u4e2a token \u4e4b\u524d\u7684\u524d\u7f00\uff1b \\(\\hat{A}_{i,t}\\) \u8868\u793a\u7b2c \\(i\\) \u4e2a output \u4e2d\u7b2c \\(t\\) \u4e2a token \u7684\u4f18\u52bf\u503c\uff1b clip(...) \u9650\u5236\u4e86ISR \u5728 \\([1-\\varepsilon, 1+\\varepsilon]\\) \u8303\u56f4\u5185\uff0c\u9632\u6b62\u7b56\u7565\u66f4\u65b0\u8fc7\u5927\uff1b \u6700\u7ec8\u53d6 min\uff0c\u8fd9\u662f\u6807\u51c6PPO \u635f\u5931\uff0c\u7528\u4e8e\u7a33\u5b9a\u8bad\u7ec3\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u91cd\u70b9\u8bf4\u660e\u7684\u662f\u4f18\u52bf\u503c\u7684\u8ba1\u7b97\u3002 \\(\\hat{A}_{i,t}\\) \u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a \u5bf9\u6bcf\u4e2a output \\(o_i\\) \uff0c\u5148\u8ba1\u7b97\u5176\u603b reward \\(r_i\\) \uff08\u5982 accuracy\u3001LLM judge \u5206\u6570\uff09\uff1b \u7136\u540e\u5bf9\u7ec4\u5185 reward \u505a\u6807\u51c6\u5316\uff1a \\[ \\hat{A}_i = \\frac{r_i - \\mu_r}{\\sigma_r + \\epsilon} \\] \u6700\u540e\u5c06 \\(\\hat{A}_i\\) \u5e7f\u64ad\u5230\u8be5 output \u7684\u6240\u6709 token \u4e0a\uff08\u5373 \\(\\hat{A}_{i,t} = \\hat{A}_i\\) \uff09\u3002\u8fd9\u91cc \\(\\epsilon\\) \u7528\u4e8e\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u3002 4. KL \u6b63\u5219\u9879\uff08\u5bf9\u9f50 && \u9632\u6b62\u9057\u5fd8\uff09 \\[ - \\beta D_{\\text{KL}}[\\pi_\\theta \\| \\pi_{\\text{ref}}] \\] \u5176\u4e2d\uff0c \\(D_{\\text{KL}}[\\pi_\\theta \\| \\pi_{\\text{ref}}]\\) \u8868\u793a\u5f53\u524d\u7b56\u7565\u4e0e\u53c2\u8003\u6a21\u578b\uff08\u901a\u5e38\u662f SFT \u6a21\u578b\uff09\u4e4b\u95f4\u7684 KL \u6563\u5ea6\uff1b \\(\\beta\\) \u4e3a\u8d85\u53c2\u6570\uff0c\u63a7\u5236\u6b63\u5219\u5f3a\u5ea6\uff1b\u51cf\u53f7\u8868\u793a\uff1a\u6211\u4eec\u5e0c\u671b\u6700\u5c0f\u5316\u8fd9\u4e2a KL\uff0c\u5373\u4e0d\u8ba9\u65b0\u7b56\u7565\u504f\u79bb\u53c2\u8003\u6a21\u578b\u592a\u8fdc\u3002\u6700\u540e\uff0c\u8ba9\u6211\u4eec\u770b\u4e00\u4e0bGRPO\u539f\u6587\u4e2d\u5bf9KL\u6563\u5ea6\u7684\u5b9a\u4e49\uff1a \\[ \\mathcal{D}_{KL}\\left[\\pi_\\theta \\mid\\mid \\pi_{\\text{ref}}\\right] = \\frac{\\pi_{\\text{ref}}(o_{i,t} \\mid q, o_{i,<t})}{\\pi_\\theta(o_{i,t} \\mid q, o_{i,<t})} - \\log \\frac{\\pi_{\\text{ref}}(o_{i,t} \\mid q, o_{i,<t})}{\\pi_\\theta(o_{i,t} \\mid q, o_{i,<t})} - 1 \\] \u4f2a\u4ee3\u7801 \u539f\u59cb\u8bba\u6587\u4e2d\u7b97\u6cd5\u6d41\u7a0b\uff1a \u4e0b\u9762\u662f\u6211\u4ecems-swift\u6587\u6863\u4e2d\u6458\u6284\u7684GRPO\u7684\u4f2a\u4ee3\u7801\uff0c\u4f9b\u5927\u5bb6\u7406\u89e3\u3002\u8fd9\u91cc\u53ef\u4ee5\u770b\u5230\uff0c\u4ee3\u7801\u4e2d\u4f1a\u5728\u539f\u59cb\u516c\u5f0f\u7684\u6574\u4f53\u52a0\u4e0a\u8d1f\u53f7\uff0c\u4ee5\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\u3002 # ========== 1. Rollout Generation Phase ========== prompt = \"Question: Which is bigger? 9.11 or 9.9?\" # Generate multiple completions through parallel sampling completions = rollout_function ( model = current_policy_model , prompt = prompt , num_generations = 8 , # Hyperparameter: number of samples per prompt temperature = 1.0 # Hyperparameter: sampling diversity ) \"\"\" completions = [ (completion 1) \"The larger number is 9.11...\", (completion 2) \"9.9 is bigger than...\", ... (completion 8) \"After calculation, 9.11...\" ] \"\"\" # ========== 2. Reward Calculation Phase ========== # Evaluate generated completions using reward model rewards = reward_function ( completions = completions , ground_truth = \"9.11\" # Expected correct answer ) \"\"\" rewards = [ (reward 1) 1.0, # Correct answer (reward 2) 0.0, # Incorrect ... (reward 8) 1.0 # Correct ] \"\"\" # Normalize rewards to advantages rewards_mean = mean ( rewards ) # \u03bc = 0.5 rewards_std = std ( rewards ) # \u03c3 = 0.25 advantages = ( rewards - rewards_mean ) / ( rewards_std + 1e-8 ) # Standardization \"\"\" advantages = [ (advantage 1) 2.0, # (1.0 - 0.5)/0.25 (advantage 2) -2.0, ... (advantage 8) 2.0 ] \"\"\" # ========== 3. Policy Optimization Phase ========== # Get token-level log probabilities from different models current_logps = get_per_token_logps ( current_policy_model , prompt , completions ) # \u03c0_\u03b8 old_logps = get_per_token_logps ( old_policy_model , prompt , completions ) # \u03c0_\u03b8_old ref_logps = get_per_token_logps ( reference_model , prompt , completions ) # \u03c0_ref # PPO Clipped Objective is_ratio = exp ( current_logps - old_logps ) # Importance sampling ratio: e^(\u03c0_\u03b8 - \u03c0_\u03b8_old) clipped_ratio = clip ( is_ratio , 1 - \u03b5 , 1 + \u03b5 ) # \u03b5=0.2 typically # Policy gradient term (dual form) policy_loss = - mean ( minimum ( is_ratio * advantages , # Unclipped objective clipped_ratio * advantages ) # Clipped objective ) # KL Divergence Penalty (K3 estimator) # KL(\u03c0_\u03b8||\u03c0_ref) \u2248 e^(log\u03c0_ref - log\u03c0_\u03b8) - (log\u03c0_ref - log\u03c0_\u03b8) - 1 kl_penalty = beta * mean ( exp ( ref_logps - current_logps ) - ( ref_logps - current_logps ) - 1 ) # Total Loss = Policy Loss + KL Penalty total_loss = policy_loss + kl_penalty # ========== 4. Update Rule ========== # Apply gradient descent to minimize total_loss optimizer . zero_grad () total_loss . backward () optimizer . step () \u53c2\u8003\u8d44\u6599 DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.(https://arxiv.org/pdf/2402.03300) https://swift.readthedocs.io/zh-cn/latest/Instruction/GRPO/GetStarted/GRPO.html Qwen & ChatGPT","title":"GRPO \u539f\u7406"},{"location":"RL/grpo/#grpogroup-relative-policy-optimization","text":"GRPO\u7684\u6838\u5fc3\u601d\u60f3\u53ef\u4ee5\u7b80\u8981\u7406\u89e3\u4e3aGrouped PPO\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u5bf9\u540c\u4e00\u4e2a\u95ee\u9898 \\(q\\) , \u4e00\u6b21\u6027\u91c7\u6837\u4e00\u7ec4\uff08 \\(G\\) \u4e2a\uff09\u5b8c\u6574\u56de\u7b54\uff0c\u7528\u7ec4\u5185\u76f8\u5bf9\u4f18\u52bf\uff08\u800c\u4e0d\u662f value baseline\uff09\u6765\u505a PPO-style \u7684 token \u7ea7\u7b56\u7565\u66f4\u65b0\uff0c\u540c\u65f6\u7528KL\u6b63\u5219\u628a\u7b56\u7565\u7ea6\u675f\u5728\u53c2\u8003\u6a21\u578b\u9644\u8fd1\u3002 \u8fd9\u91cc\u7684Group\u6307\u7684\u5c31\u662f\u5206\u7ec4\uff1bRelative\u6307\u7684\u5c31\u662f\u7ec4\u5185\u76f8\u5bf9\u4f18\u52bf\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u60f3\u6839\u636eGRPO\u4e2d\u6d89\u53ca\u5230\u7684\u516c\u5f0f\uff0c\u548c\u5bf9\u5e94\u7684\u4f2a\u4ee3\u7801\uff0c\u6df1\u5165\u7406\u89e3\u4e00\u4e0bGRPO\u91cc\u9762\u7684\u7ec6\u8282\uff1b\u5982\u679c\u4f60\u5bf9\u516c\u5f0f\u4e0d\u611f\u5174\u8da3\uff0c\u90a3\u4e48\u53ef\u4ee5\u76f4\u63a5\u8df3\u8fc7\u516c\u5f0f\u56de\u987e\u548c\u6df1\u5165\u7406\u89e3\u7684\u90e8\u5206\u3002","title":"GRPO\uff08Group Relative Policy Optimization\uff09\u7406\u89e3"},{"location":"RL/grpo/#_1","text":"\u539f\u6587\u4e2d\u516c\u5f0f\u5982\u4e0b\uff1a \\[ \\begin{aligned} \\mathcal{J}_{G R P O}(\\theta) & =\\mathbb{E}_{\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right]} \\\\ & \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|}\\left\\{\\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)} \\hat{A}_{i, t}, \\operatorname{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i, t} \\mid q, o_{i,<t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]-\\beta \\mathbb{D}_{K L}\\left[\\pi_\\theta| | \\pi_{r e f}\\right]\\right\\} \\end{aligned} % \\tag{1} \\] \u6211\u611f\u89c9\u4f5c\u8005\u7701\u7565\u4e86\u4e00\u5bf9\u62ec\u53f7\uff0c\u5bf9\u6570\u5b66\u4e0d\u597d\u7684\u540c\u5b66\u4e0d\u592a\u53cb\u597d\uff0c\u56e0\u6b64\u8fd9\u91cc\u7ed9\u5b83\u52a0\u4e0a\uff0c\u957f\u6210\u8fd9\u6837\uff1a \\[ \\begin{aligned} \\mathcal{J}_{\\text{GRPO}}(\\theta) &= \\mathbb{E}_{\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(o|q)\\right]} \\Bigg[ \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\Bigg\\{ \\min \\Bigg[ \\frac{\\pi_\\theta(o_{i,t}\\mid q, o_{i,<t})} {\\pi_{\\theta_{\\text{old}}}(o_{i,t}\\mid q, o_{i,<t})} \\hat{A}_{i,t}, \\\\ &\\quad\\quad \\operatorname{clip}\\!\\left( \\frac{\\pi_\\theta(o_{i,t}\\mid q, o_{i,<t})} {\\pi_{\\theta_{\\text{old}}}(o_{i,t}\\mid q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\Bigg] - \\beta \\, D_{\\text{KL}}\\!\\left[\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\right] \\Bigg\\} \\Bigg] \\end{aligned} \\] \u6574\u4e2a GRPO \u76ee\u6807\u51fd\u6570 \\(\\mathcal{J}_{\\text{GRPO}}(\\theta)\\) \u662f\u4e00\u4e2a\u671f\u671b\u6536\u76ca\uff08expected objective\uff09\uff0c\u5373\u6a21\u578b\u7b56\u7565 \\(\\pi_\\theta\\) \u5728\u5927\u91cf\u4e0d\u540c\u4efb\u52a1\uff08prompts\uff09\u548c\u751f\u6210\u7ed3\u679c(responses)\u4e0a\u7684\u6574\u4f53\u8868\u73b0\u6c34\u5e73\uff08\u7531 group-normalized advantage \u52a0\u6743\uff0c\u5e76\u51cf\u53bb KL \u6b63\u5219\u9879\u8868\u793a\uff09\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\uff1a\u901a\u8fc7\u8c03\u6574 \\(\\theta\\) \uff0c\u8ba9\u8fd9\u4e2a\u671f\u671b\u6536\u76ca\u5c3d\u53ef\u80fd\u5927\u3002 \u6ce8\uff1a\u5728\u5b9e\u9645\u8bad\u7ec3\u4e2d\uff0c\u4e3a\u4e86\u517c\u5bb9\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u5e38\u505a\u6cd5\u662f\u6700\u5c0f\u5316\u5176\u8d1f\u503c\uff0c\u5373\u5b9a\u4e49\uff1a $$ \\mathcal{L} {\\text{GRPO}}(\\theta) = -\\mathcal{J} {\\text{GRPO}}(\\theta) $$ \u5e76\u5bf9 \\(\\mathcal{L}_{\\text{GRPO}}(\\theta)\\) \u6267\u884c\u68af\u5ea6\u4e0b\u964d\u3002","title":"\u516c\u5f0f\u56de\u987e"},{"location":"RL/grpo/#_2","text":"","title":"\u6df1\u5165\u7406\u89e3"},{"location":"RL/grpo/#1-expectation","text":"\\[ \\mathbb{E}_{[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(o|q)]} \\] \u5176\u4e2d, \\(q\\) \u8868\u793a\u4e00\u4e2a prompt\uff08\u95ee\u9898\uff09\uff0c\u4ece\u6570\u636e\u5206\u5e03 \\(P(Q)\\) \u4e2d\u91c7\u6837\uff0c \\(P(Q)\\) \u5305\u542b\u4e86\u4e0d\u540c\u7c7b\u578b\u7684prompt\uff0c\u4f8b\u5982\u533b\u5b66\u95ee\u9898\uff0cVQA\u3001\u62a5\u544a\u751f\u6210\u7b49\uff1b \\(\\{o_i\\}_{i=1}^G\\) \u8868\u793a\u5bf9\u540c\u4e00\u4e2a \\(q\\) \uff0c\u7528\u65e7\u7b56\u7565 \\(\\pi_{\\theta_{\\text{old}}}\\) \u751f\u6210\u7684 G \u4e2a\u8f93\u51fa\uff08completions\uff09 . \u4e5f\u5c31\u662f\u8bf4\uff1a \u5bf9\u6bcf\u4e2a prompt\uff0c\u6211\u4eec\u5e76\u884c\u751f\u6210 G \u4e2a\u56de\u7b54\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u56de\u7b54\u505a\u4f18\u5316 . \u8fd9\u5c31\u662f GRPO \u7684\u201c\u7ec4\u5185\u5bf9\u6bd4\u201d\u601d\u60f3. \u8fdb\u4e00\u6b65\u7406\u89e3\uff0c\u8fd9\u4e2a\u671f\u671b\u53ef\u4ee5\u8868\u793a\uff1a\"\u5982\u679c\u6211\u4eec\u4ece\u771f\u5b9e prompt \u5206\u5e03\u4e2d\u968f\u673a\u6311\u4e00\u4e2a \\(q\\) \uff0c\u518d\u7528\u65e7\u7b56\u7565\u5bf9\u5b83\u751f\u6210 \\(G\\) \u4e2a\u968f\u673a\u56de\u7b54\uff0c\u7136\u540e\u8ba1\u7b97 GRPO loss\uff0c\u90a3\u4e48\u8fd9\u4e2a loss \u7684\u5e73\u5747\u503c\u662f\u591a\u5c11?\"\uff0c\u800c\u6211\u4eec\u7684\u76ee\u6807\u662f\uff1a\u901a\u8fc7\u8c03\u6574 \\(\\theta\\) \uff08\u5f53\u524d\u7b56\u7565\uff09\uff0c\u8ba9\u8fd9\u4e2a\u671f\u671b loss \u5c3d\u53ef\u80fd\u5c0f\u3002\" \u8fd9\u91cc\u6211\u4eec\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u6765\u505a\u4e2a\u7c7b\u6bd4\uff0c\u76d1\u7763\u5b66\u4e60\u7684\u76ee\u6807\u51fd\u6570\u5982\u4e0b\uff1a \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{(x, y) \\sim \\mathcal{D}} \\left[ \\text{CrossEntropy}(f_\\theta(x), y) \\right] \\] \u5176\u4e2d \\(\\mathcal{D}\\) \u8868\u793a\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c \\((x, y)\\) \u8868\u793a\u4e00\u6761\u6837\u672c\uff08\u5982\u56fe\u50cf+\u6807\u7b7e\uff09\uff0cCrossEntropy\u8868\u793a\u5bf9\u8fd9\u6761\u6837\u672c\u8ba1\u7b97\u7684 loss\u3002\u90a3\u4e48\u8fd9\u4e2a\u516c\u5f0f\u7684\u6574\u4f53\u542b\u4e49\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\u5728\u6240\u6709\u53ef\u80fd\u7684\u6570\u636e\u4e0a\uff0c\u6a21\u578b\u9884\u6d4b\u7684\u5e73\u5747\u4ea4\u53c9\u71b5\u635f\u5931\u3002 \u4f46\u6211\u4eec\u65e0\u6cd5\u8ba1\u7b97\u771f\u5b9e\u671f\u671b\uff08\u56e0\u4e3a \\(\\mathcal{D}\\) \u672a\u77e5\uff09\uff0c\u6240\u4ee5\u7528 Monte Carlo \u4f30\u8ba1 \uff08\u5373 mini-batch\uff09\uff1a \\[ \\hat{\\mathcal{L}}(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\text{CrossEntropy}(f_\\theta(x_i), y_i) \\] \u4e5f\u5c31\u662f\u8bf4\uff0c \u671f\u671b = \u7406\u8bba\u76ee\u6807\uff0c\u6c42\u548c = \u5b9e\u9645\u8fd1\u4f3c \u3002\u56de\u5230 GRPO,\u5b8c\u5168\u4e00\u6837\u7684\u903b\u8f91,\u5bf9\u5e94\u5173\u7cfb\u5982\u4e0b\uff1a \u76d1\u7763\u5b66\u4e60 GRPO \u6570\u636e\u5206\u5e03 \\(\\mathcal{D}\\) Prompt \u5206\u5e03 \\(P(Q)\\) + \u65e7\u7b56\u7565\u751f\u6210\u5206\u5e03 \\(\\pi_{\\theta_{\\text{old}}}\\) \u4e00\u6761\u6837\u672c \\((x, y)\\) \u4e00\u7ec4\u6570\u636e \\((q, o_1, ..., o_G)\\) Loss \u51fd\u6570\uff08\u5982 CE\uff09 PPO-style clipped objective + KL penalty \u671f\u671b \\(\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}[\\text{loss}]\\) \\(\\mathbb{E}_{q, \\{o_i\\}}[\\text{GRPO loss}]\\)","title":"1. \u6700\u5916\u5c42\u671f\u671b\uff08Expectation\uff09\uff1a\u5728\u4ec0\u4e48\u5206\u5e03\u4e0a\u8fdb\u884c\u4f18\u5316\uff1f"},{"location":"RL/grpo/#2-token","text":"\u4ece\u4e0b\u9762\u7684\u516c\u5f0f\uff0c\u53ef\u4ee5\u770b\u5230\u8fd9\u91cc\u6709\u4e24\u5c42\u5e73\u5747\uff0c\u5916\u5c42\u5e73\u5747\u8868\u793a\u5bf9\u4e00\u4e2aprompt\u4ea7\u751f\u7684\u5206\u7ec4\u5185\u90e8loss\u505a\u5e73\u5747\uff0c\u907f\u514d \\(G\\) \u53d8\u5927\u5bfc\u81f4\u68af\u5ea6\u7206\u70b8\uff1b\u5185\u5c42\u5bf9token\u518d\u5e73\u5747\uff0c\u4f7f\u5f97\u4e0d\u540c\u957f\u5ea6\u7684\u56de\u7b54\u88ab\u5f52\u4e00\u5316\u5230\u76f8\u540c\u7684\u5c3a\u5ea6\uff0c\u9632\u6b62\u6a21\u578b\u751f\u6210\u66f4\u957f\u7684token\u6765\u5237reward\u3002 \\[ \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left\\{ \\cdots \\right\\} \\] \u5177\u4f53\u5730\uff0c\u5bf9\u4e8e\u6bcf\u4e2a output \\(o_i\\) \uff1a\u8ba1\u7b97\u5176\u6bcf\u4e2a token \\(t\\) \u5bf9\u5e94\u7684 loss\uff0c\u5176\u4e2d \\(|o_i|\\) \u8868\u793a \\(o_i\\) \u7684\u5e8f\u5217\u957f\u5ea6\uff1b \u7136\u540e\u5bf9\u6240\u6709 token \u53d6\u5e73\u5747\uff08 \\(\\frac{1}{|o_i|}\\) \uff09\uff1b \u518d\u5bf9\u6240\u6709 G \u4e2a output \u53d6\u5e73\u5747\uff08 \\(\\frac{1}{G}\\) \uff09\u3002","title":"2. \u5206\u7ec4\u5e73\u5747 &amp;&amp; Token\u5e73\u5747\uff1a\u4fdd\u6301\u68af\u5ea6\u7a33\u5b9a\u7684\u540c\u65f6\u62c9\u9f50\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6\u7684\u8d21\u732e"},{"location":"RL/grpo/#3-ppo-style-clipped-objective","text":"\u63a5\u4e0b\u6765\u6211\u4eec\u770b\u4e00\u4e0b\u5927\u62ec\u53f7\u5185\u90e8\uff0c\u5b9e\u9645\u4e0a\u662f\u65b0\u65e7\u7b56\u7565\u4e4b\u95f4\u7684token\u7ea7\u91cd\u8981\u6027\u91c7\u6837\u6bd4(Importance Sampling Ration, ISR)\u548c\u4f18\u52bf\u503c(Advantage)\u7684\u4e58\u79ef\uff0c\u5177\u4f53\u5730\uff1a \\[ \\min \\left[ \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\, \\text{clip}\\left( \\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon \\right) \\hat{A}_{i,t} \\right] \\] \u5176\u4e2d\uff0c \\(\\pi_{\\theta}\\) \u8868\u793a\u5f53\u524d\u5f85\u4f18\u5316\u7684\u7b56\u7565\u6a21\u578b\uff08policy\uff09\uff0c \\(\\frac{\\pi_\\theta}{\\pi_{\\text{old}}}\\) \u4e3aISR\uff0c \\(o_{i,t}\\) \u8868\u793a\u7b2c \\(i\\) \u4e2a\u56de\u7b54\u7684\u7b2c \\(t\\) \u4e2a token\uff0c \\(o_{i,<t}\\) \u8868\u793a\u7b2c \\(i\\) \u4e2a\u56de\u7b54\u4e2d\u7b2c \\(t\\) \u4e2a token \u4e4b\u524d\u7684\u524d\u7f00\uff1b \\(\\hat{A}_{i,t}\\) \u8868\u793a\u7b2c \\(i\\) \u4e2a output \u4e2d\u7b2c \\(t\\) \u4e2a token \u7684\u4f18\u52bf\u503c\uff1b clip(...) \u9650\u5236\u4e86ISR \u5728 \\([1-\\varepsilon, 1+\\varepsilon]\\) \u8303\u56f4\u5185\uff0c\u9632\u6b62\u7b56\u7565\u66f4\u65b0\u8fc7\u5927\uff1b \u6700\u7ec8\u53d6 min\uff0c\u8fd9\u662f\u6807\u51c6PPO \u635f\u5931\uff0c\u7528\u4e8e\u7a33\u5b9a\u8bad\u7ec3\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u8fd8\u9700\u8981\u91cd\u70b9\u8bf4\u660e\u7684\u662f\u4f18\u52bf\u503c\u7684\u8ba1\u7b97\u3002 \\(\\hat{A}_{i,t}\\) \u7684\u8ba1\u7b97\u65b9\u5f0f\u5982\u4e0b\uff1a \u5bf9\u6bcf\u4e2a output \\(o_i\\) \uff0c\u5148\u8ba1\u7b97\u5176\u603b reward \\(r_i\\) \uff08\u5982 accuracy\u3001LLM judge \u5206\u6570\uff09\uff1b \u7136\u540e\u5bf9\u7ec4\u5185 reward \u505a\u6807\u51c6\u5316\uff1a \\[ \\hat{A}_i = \\frac{r_i - \\mu_r}{\\sigma_r + \\epsilon} \\] \u6700\u540e\u5c06 \\(\\hat{A}_i\\) \u5e7f\u64ad\u5230\u8be5 output \u7684\u6240\u6709 token \u4e0a\uff08\u5373 \\(\\hat{A}_{i,t} = \\hat{A}_i\\) \uff09\u3002\u8fd9\u91cc \\(\\epsilon\\) \u7528\u4e8e\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u3002","title":"3. PPO-style Clipped Objective"},{"location":"RL/grpo/#4-kl","text":"\\[ - \\beta D_{\\text{KL}}[\\pi_\\theta \\| \\pi_{\\text{ref}}] \\] \u5176\u4e2d\uff0c \\(D_{\\text{KL}}[\\pi_\\theta \\| \\pi_{\\text{ref}}]\\) \u8868\u793a\u5f53\u524d\u7b56\u7565\u4e0e\u53c2\u8003\u6a21\u578b\uff08\u901a\u5e38\u662f SFT \u6a21\u578b\uff09\u4e4b\u95f4\u7684 KL \u6563\u5ea6\uff1b \\(\\beta\\) \u4e3a\u8d85\u53c2\u6570\uff0c\u63a7\u5236\u6b63\u5219\u5f3a\u5ea6\uff1b\u51cf\u53f7\u8868\u793a\uff1a\u6211\u4eec\u5e0c\u671b\u6700\u5c0f\u5316\u8fd9\u4e2a KL\uff0c\u5373\u4e0d\u8ba9\u65b0\u7b56\u7565\u504f\u79bb\u53c2\u8003\u6a21\u578b\u592a\u8fdc\u3002\u6700\u540e\uff0c\u8ba9\u6211\u4eec\u770b\u4e00\u4e0bGRPO\u539f\u6587\u4e2d\u5bf9KL\u6563\u5ea6\u7684\u5b9a\u4e49\uff1a \\[ \\mathcal{D}_{KL}\\left[\\pi_\\theta \\mid\\mid \\pi_{\\text{ref}}\\right] = \\frac{\\pi_{\\text{ref}}(o_{i,t} \\mid q, o_{i,<t})}{\\pi_\\theta(o_{i,t} \\mid q, o_{i,<t})} - \\log \\frac{\\pi_{\\text{ref}}(o_{i,t} \\mid q, o_{i,<t})}{\\pi_\\theta(o_{i,t} \\mid q, o_{i,<t})} - 1 \\]","title":"4. KL \u6b63\u5219\u9879\uff08\u5bf9\u9f50 &amp;&amp; \u9632\u6b62\u9057\u5fd8\uff09"},{"location":"RL/grpo/#_3","text":"\u539f\u59cb\u8bba\u6587\u4e2d\u7b97\u6cd5\u6d41\u7a0b\uff1a \u4e0b\u9762\u662f\u6211\u4ecems-swift\u6587\u6863\u4e2d\u6458\u6284\u7684GRPO\u7684\u4f2a\u4ee3\u7801\uff0c\u4f9b\u5927\u5bb6\u7406\u89e3\u3002\u8fd9\u91cc\u53ef\u4ee5\u770b\u5230\uff0c\u4ee3\u7801\u4e2d\u4f1a\u5728\u539f\u59cb\u516c\u5f0f\u7684\u6574\u4f53\u52a0\u4e0a\u8d1f\u53f7\uff0c\u4ee5\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\u3002 # ========== 1. Rollout Generation Phase ========== prompt = \"Question: Which is bigger? 9.11 or 9.9?\" # Generate multiple completions through parallel sampling completions = rollout_function ( model = current_policy_model , prompt = prompt , num_generations = 8 , # Hyperparameter: number of samples per prompt temperature = 1.0 # Hyperparameter: sampling diversity ) \"\"\" completions = [ (completion 1) \"The larger number is 9.11...\", (completion 2) \"9.9 is bigger than...\", ... (completion 8) \"After calculation, 9.11...\" ] \"\"\" # ========== 2. Reward Calculation Phase ========== # Evaluate generated completions using reward model rewards = reward_function ( completions = completions , ground_truth = \"9.11\" # Expected correct answer ) \"\"\" rewards = [ (reward 1) 1.0, # Correct answer (reward 2) 0.0, # Incorrect ... (reward 8) 1.0 # Correct ] \"\"\" # Normalize rewards to advantages rewards_mean = mean ( rewards ) # \u03bc = 0.5 rewards_std = std ( rewards ) # \u03c3 = 0.25 advantages = ( rewards - rewards_mean ) / ( rewards_std + 1e-8 ) # Standardization \"\"\" advantages = [ (advantage 1) 2.0, # (1.0 - 0.5)/0.25 (advantage 2) -2.0, ... (advantage 8) 2.0 ] \"\"\" # ========== 3. Policy Optimization Phase ========== # Get token-level log probabilities from different models current_logps = get_per_token_logps ( current_policy_model , prompt , completions ) # \u03c0_\u03b8 old_logps = get_per_token_logps ( old_policy_model , prompt , completions ) # \u03c0_\u03b8_old ref_logps = get_per_token_logps ( reference_model , prompt , completions ) # \u03c0_ref # PPO Clipped Objective is_ratio = exp ( current_logps - old_logps ) # Importance sampling ratio: e^(\u03c0_\u03b8 - \u03c0_\u03b8_old) clipped_ratio = clip ( is_ratio , 1 - \u03b5 , 1 + \u03b5 ) # \u03b5=0.2 typically # Policy gradient term (dual form) policy_loss = - mean ( minimum ( is_ratio * advantages , # Unclipped objective clipped_ratio * advantages ) # Clipped objective ) # KL Divergence Penalty (K3 estimator) # KL(\u03c0_\u03b8||\u03c0_ref) \u2248 e^(log\u03c0_ref - log\u03c0_\u03b8) - (log\u03c0_ref - log\u03c0_\u03b8) - 1 kl_penalty = beta * mean ( exp ( ref_logps - current_logps ) - ( ref_logps - current_logps ) - 1 ) # Total Loss = Policy Loss + KL Penalty total_loss = policy_loss + kl_penalty # ========== 4. Update Rule ========== # Apply gradient descent to minimize total_loss optimizer . zero_grad () total_loss . backward () optimizer . step ()","title":"\u4f2a\u4ee3\u7801"},{"location":"RL/grpo/#_4","text":"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.(https://arxiv.org/pdf/2402.03300) https://swift.readthedocs.io/zh-cn/latest/Instruction/GRPO/GetStarted/GRPO.html Qwen & ChatGPT","title":"\u53c2\u8003\u8d44\u6599"},{"location":"RL/ppo/","text":"PPO \u539f\u7406 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"PPO \u539f\u7406"},{"location":"RL/ppo/#ppo","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"PPO \u539f\u7406"},{"location":"dist/data_parallel/","text":"\u6570\u636e\u5e76\u884c TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u6570\u636e\u5e76\u884c"},{"location":"dist/data_parallel/#_1","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u6570\u636e\u5e76\u884c"},{"location":"dist/gradient_accumulation/","text":"\u68af\u5ea6\u7d2f\u79ef TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u68af\u5ea6\u7d2f\u79ef"},{"location":"dist/gradient_accumulation/#_1","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u68af\u5ea6\u7d2f\u79ef"},{"location":"dist/mixed_precision/","text":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3"},{"location":"dist/mixed_precision/#_1","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3"},{"location":"dist/model_parallel/","text":"\u6a21\u578b\u5e76\u884c TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u6a21\u578b\u5e76\u884c"},{"location":"dist/model_parallel/#_1","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u6a21\u578b\u5e76\u884c"},{"location":"dist/parallel_training_optimization/","text":"\u5e76\u884c\u8bad\u7ec3\u4f18\u5316 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u5e76\u884c\u8bad\u7ec3\u4f18\u5316"},{"location":"dist/parallel_training_optimization/#_1","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u5e76\u884c\u8bad\u7ec3\u4f18\u5316"},{"location":"dist/pipeline_parallel/","text":"\u6d41\u6c34\u7ebf\u5e76\u884c TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u6d41\u6c34\u7ebf\u5e76\u884c"},{"location":"dist/pipeline_parallel/#_1","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u6d41\u6c34\u7ebf\u5e76\u884c"},{"location":"model/deepseek-r1/","text":"DeepSeek\u7cfb\u5217\u6a21\u578b\u89e3\u6790 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"DeepSeek\u7cfb\u5217\u6a21\u578b\u89e3\u6790"},{"location":"model/deepseek-r1/#deepseek","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"DeepSeek\u7cfb\u5217\u6a21\u578b\u89e3\u6790"},{"location":"model/intervl/","text":"InterVL\u7cfb\u5217\u6a21\u578b\u89e3\u6790 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"InterVL\u7cfb\u5217\u6a21\u578b\u89e3\u6790"},{"location":"model/intervl/#intervl","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"InterVL\u7cfb\u5217\u6a21\u578b\u89e3\u6790"},{"location":"model/qwen-vl/","text":"Qwen-VL\u7cfb\u5217\u6a21\u578b\u89e3\u6790 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"Qwen-VL\u7cfb\u5217\u6a21\u578b\u89e3\u6790"},{"location":"model/qwen-vl/#qwen-vl","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"Qwen-VL\u7cfb\u5217\u6a21\u578b\u89e3\u6790"},{"location":"transformer/attention/","text":"Attention \u673a\u5236 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"Attention \u673a\u5236"},{"location":"transformer/attention/#attention","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"Attention \u673a\u5236"},{"location":"transformer/kv_cache/","text":"KV Cache \u539f\u7406 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"KV Cache \u539f\u7406"},{"location":"transformer/kv_cache/#kv-cache","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"KV Cache \u539f\u7406"},{"location":"transformer/multi_head_attention/","text":"\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236"},{"location":"transformer/multi_head_attention/#_1","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236"},{"location":"transformer/position_encoding/","text":"\u4f4d\u7f6e\u7f16\u7801 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u4f4d\u7f6e\u7f16\u7801"},{"location":"transformer/position_encoding/#_1","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u4f4d\u7f6e\u7f16\u7801"},{"location":"transformer/rope/","text":"test ROPE","title":"RoPE \u539f\u7406"},{"location":"transformer/self_attention/","text":"\u81ea\u6ce8\u610f\u529b\u673a\u5236 TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u81ea\u6ce8\u610f\u529b\u673a\u5236"},{"location":"transformer/self_attention/#_1","text":"TODO: \u5f85\u8865\u5145\u5185\u5bb9","title":"\u81ea\u6ce8\u610f\u529b\u673a\u5236"}]}