# 多模态大模型架构

多模态大模型（MLLM/VLM）的核心目标是：让模型能够在“看（视觉/视频/语音）+ 读（文本）”的联合条件下进行理解与生成。工程上你会反复遇到三个问题：

1. **表示**：不同模态如何编码成可用的 token/向量？
2. **对齐**：视觉与语言语义如何对齐（数据、目标函数、训练策略）？
3. **融合**：在哪里、以什么机制让模态交互（early/late/cross-attn）？

## 单塔 vs 双塔结构

这是最常见的架构分岔点。

### 双塔（Two-tower / Dual-encoder）

典型代表：CLIP。

- 图像 encoder：\(z_v = f_v(\text{image})\)
- 文本 encoder：\(z_t = f_t(\text{text})\)
- 用相似度（cosine/dot-product）做对比学习

优点：

- 检索任务非常强（图搜文/文搜图）
- 编码可离线缓存，在线成本低

缺点：

- 交互弱，不擅长复杂推理与逐步生成

### 单塔（Single-tower / Unified）

典型代表：把视觉 token 作为条件输入喂给 LLM（BLIP-2、Flamingo、许多现代 MLLM）。

优点：

- 交互强，适合 VQA、对话、生成式任务
- 可以复用 LLM 的推理与生成能力

缺点：

- 推理成本更高（cross-attn/长序列）
- 更依赖高质量对齐数据与训练配方
## 融合策略

### Cross-Attention

Cross-Attention 是“一个模态 attend 另一个模态”的通用机制。最常见的是：

- 语言 token 作为 query
- 视觉 token 作为 key/value

形式上：

\[
\mathrm{Attn}(Q_t, K_v, V_v)=\mathrm{softmax}\left(\frac{Q_t K_v^\top}{\sqrt{d}}+M\right)V_v
\]

直觉：语言在生成/推理过程中“按需从视觉记忆里检索信息”。

工程要点：

- 视觉 token 数量（patch 数）直接影响计算量与延迟
- 视觉 encoder 是否冻结、视觉特征是否缓存会显著影响训练与推理成本
### Co-Attention

Co-Attention 常指双向交互：视觉 attend 文本、文本 attend 视觉，或在中间层做更对称的融合。

优点是信息流更充分，缺点是实现更复杂、成本更高。在很多工程系统里，会优先选择更“单向可控”的 cross-attn，再通过层数与 token 数来调节能力/成本。
## 多模态预训练任务

### Masked Language Modeling

在多模态中，MLM 常用于：

- 让语言端学习更稳健的上下文表示
- 配合视觉条件进行“有条件的补全”

但现代 MLLM 更常把主目标统一成“条件自回归生成”（因为最终产品形态往往是对话/生成）。
### Image-Text Contrastive Learning

这是 CLIP 系的核心。最常见的 loss 是 InfoNCE：

\[
\mathcal{L}=-\log \frac{\exp(\mathrm{sim}(z_v, z_t)/\tau)}{\sum_{t'} \exp(\mathrm{sim}(z_v, z_{t'})/\tau)}
\]

其中 \(\tau\) 是温度系数。

工程要点：

- batch 越大，对比学习的负样本越多，通常越有效
- 数据噪声与“伪对齐”会显著伤害表示质量（caption 模板化、错配等）
### Vision-Language Generation

把任务统一成条件生成（image + text prompt → answer/caption），通常用序列交叉熵训练：

\[
\mathcal{L}(\theta) = -\sum_t \log p_\theta(y_t \mid y_{<t}, \text{image}, \text{prompt})
\]

这条路线的关键在于：视觉信息如何注入 LLM（视觉 token、cross-attn、prefix 等）以及对齐数据与指令数据如何组织。

---

## 多模态理解 vs 多模态生成：目标、训练与评测的差异

在工程落地时，“理解（understanding）”和“生成（generation）”经常被混在一起讨论，但它们对模型与系统的要求不同：

### 多模态理解（Understanding）：更强调“可验证的正确性”

典型问题：

- 这张图里有什么？（分类/检测/识别/OCR）
- 问题的答案是什么？（VQA、视频问答）
- 这段视频发生了什么？（事件、动作、时序关系）

常见训练信号：

- 对比学习（图文检索）
- 结构化监督（label/box/span）
- 指令微调（把理解任务转成可控输出格式）

评测与落地要点：

- **可验证性**：尽量把输出做成结构化（标签/坐标/引用区域/证据句）而不是一段自然语言
- **鲁棒性**：对分辨率、裁剪、压缩噪声、OCR 错误的稳定性
- **幻觉风险**：理解任务中“看图胡说”通常是致命问题，必须引入校验与拒答策略

### 多模态生成（Generation）：更强调“表达能力与一致性”

典型问题：

- 图像描述/视频字幕（captioning）
- 视觉条件对话（image/video grounded chat）
- 文本→图像/视频生成（生成式模型分支，或统一成 token 生成）

常见训练信号：

- 条件自回归生成（cross-entropy）
- 偏好对齐（DPO/RLHF/GRPO：更像“输出质量/安全/风格”的约束）

评测与落地要点：

- **事实一致性（groundedness）**：生成内容必须能回到输入模态证据上
- **多样性 vs 确定性**：温度/采样策略会显著影响“好看但不准/很准但啰嗦”
- **长上下文与多轮对话**：需要缓存视觉特征与 KV cache，且要有上下文整理策略

### 一个工程结论：理解优先“结构化 + 可验证”，生成优先“证据化 + 可控”

如果目标是“严谨可用”，推荐的系统化做法是：

- 先做理解：提取结构化事实与证据（区域/文本引用/时间片段）
- 再做生成：基于证据组织语言（并显式引用证据来源）

这样能显著降低多模态幻觉，并让后续评测与迭代更可控。
## 典型模型案例

### CLIP / BLIP / Flamingo

你可以把它们看成三条代表路线：

- **CLIP**：双塔 + 对比学习 → 强检索/表征
- **BLIP/BLIP-2**：更关注对齐与生成，常见做法是用桥接模块把视觉特征接入 LLM
- **Flamingo**：用 cross-attn 把视觉 token 注入语言层，并强调 few-shot 多图多轮能力
### Qwen-VL 系列

工程上常见特点：

- 强指令化与对话式接口
- 视觉编码器 + 语言主干的系统化设计
- 对中文/多语言支持更友好（取决于词表与数据）
### InterVL 系列

通常更强调：

- 多模态对齐与评测体系
- 在多个 VQA/检索/生成基准上的均衡性能

（后续可以在这里补“训练配方、数据构成、结构差异”的对比表。）
### DeepSeek 系列

这里建议把重点放在“训练与对齐方法论”对多模态的迁移：例如 reward 设计、数据闭环、推理系统工程等。

---

## 本章小结

- 双塔强检索，单塔强交互与生成；选择取决于任务与成本约束。
- 融合的关键是 cross-attn 的 token 数与层数，以及数据对齐质量。
- 预训练任务从对比学习到条件生成各有侧重，工程系统常混合使用。