# 强化学习与指令调优

> 本章介绍强化学习在大模型对齐中的应用

## 指令微调 (Instruction Fine-Tuning, SFT)

SFT 的目标是把“会续写的预训练模型”变成“能遵循指令的助手模型”。它通常是对齐流水线的第一步，也是后续 RLHF/GRPO 等方法的基座（reference / 初始化）。

### 1) 问题形式：把任务统一成条件生成

对每条指令数据，我们构造输入 \(x\)（system + user + history）与目标输出 \(y\)（assistant answer），用 teacher forcing 训练：

\[
\mathcal{L}_{\text{SFT}}(\theta) = -\sum_{t=1}^{|y|} \log p_\theta(y_t \mid x, y_{<t})
\]

工程上关键不是公式，而是**数据协议**：

- system prompt 里约束角色与风格
- user prompt 表达任务
- assistant 输出要尽量结构化、可验证（尤其在高风险任务）

### 2) 数据：决定“助手长什么样”

常见数据类型：

- 指令-回答（single-turn）
- 多轮对话（multi-turn）
- 结构化输出（JSON/表格/代码）与格式约束

经验法则：

- 数据质量 > 数据量（对齐数据噪声会直接变成模型行为）
- 覆盖“拒答/澄清/引用证据”这些行为，否则后续 RL 更难补齐

### 3) 工程配方要点（最常见的坑）

- **过拟合与遗忘**：只用指令数据会让模型遗忘预训练能力；实践中会混入部分预训练数据，或控制学习率与训练步数。
- **长度与截断**：对话历史过长会被截断，导致训练信号不一致；需要明确 truncation 策略（优先保留 system 与最近轮次）。
- **损失掩码**：通常只对 assistant 片段计算 loss，避免模型学会“复读用户输入”。
- **评测**：SFT 的离线评测建议包含结构化校验（格式、可执行性、引用），而不是只看生成相似度。

## 强化学习策略

### PPO 原理

在对齐里，PPO（Proximal Policy Optimization）的核心是：在不让策略更新“跳太远”的前提下，使用偏好/奖励信号提升输出质量，并用 KL 约束保持与参考模型（通常是 SFT 模型）一致，从而避免发散与灾难性遗忘。

### 1) 基本对象：policy / ref / reward

- \(\pi_\theta\)：当前策略模型（要优化）
- \(\pi_{\text{ref}}\)：参考模型（通常冻结的 SFT 模型）
- \(r(x,y)\)：对给定输入 \(x\) 与输出 \(y\) 的奖励（来自 RM/规则/评测器）

### 2) 目标：带 KL 正则的策略优化

对齐里常用的形式是最大化期望奖励，同时惩罚偏离参考策略：

\[
\max_\theta \ \mathbb{E}_{y \sim \pi_\theta(\cdot \mid x)}\Big[r(x,y)\Big] - \beta \, \mathbb{E}\big[D_{\text{KL}}(\pi_\theta \,\|\, \pi_{\text{ref}})\big]
\]

其中 \(\beta\) 控制“对齐强度”（越大越保守，越小越激进）。

### 3) PPO 的 clipped objective：限制每步更新幅度

定义重要性采样比（token 级或序列级）：

\[
\rho_t(\theta)=\exp\big(\log \pi_\theta(a_t\mid s_t)-\log \pi_{\theta_{\text{old}}}(a_t\mid s_t)\big)
\]

PPO 的核心是对 policy gradient 项做裁剪：

\[
\mathcal{L}_{\text{PPO}}(\theta) = -\mathbb{E}\Big[\min\big(\rho_t(\theta)\hat{A}_t,\ \text{clip}(\rho_t(\theta), 1-\epsilon,1+\epsilon)\hat{A}_t\big)\Big]
\]

直觉：如果新策略相对旧策略变化太大，就把收益项裁掉，避免不稳定的大步更新。

### 4) 优势函数（Advantage）：奖励如何分配到 token？

LLM 对齐里常见做法是“序列级奖励 → token 级广播”，即对一个回答得到一个总奖励 \(r\)，再估计优势 \(\hat{A}\) 并分配到每个 token。

是否引入 value baseline（critic）取决于方法：

- PPO 通常训练 value head 来降低方差
- GRPO 用组内相对优势替代 value baseline（见子章节）

### 5) 工程实现要点（决定成败）

- **KL 的估计与控制**：KL 爆掉通常意味着策略发散；需要 KL target、动态调 \(\beta\)、以及稳定的 KL 估计器。
- **reward hacking**：模型会钻奖励函数漏洞；必须把 reward 拆成可验证子项，并做对抗样本回归。
- **采样策略**：temperature/top-p 会影响探索；太贪婪会学不到改进，太随机会引入噪声与不稳定。
- **离线/在线一致性**：离线 reward 指标提升不代表线上可用，需要把“可控失败/拒答/引用”纳入评测闭环。

### GRPO 原理

详见子章节：`GRPO（Grouped PPO）理解` ✅

```{toctree}
:maxdepth: 1

grpo
```

## Reward 模型与评价指标

对齐的本质是：你用什么“奖励/评价”定义“好答案”，模型就会朝那个方向优化。因此 reward 设计与评测体系往往比算法名词更关键。

### 1) Reward 的来源

常见三类：

- **Reward Model（RM）**：人类偏好数据（pairwise/排序）训练出的评分模型
- **规则/可执行校验**：格式校验、单元测试、执行结果（代码/SQL/工具调用）
- **LLM-as-a-Judge**：用更强模型做评审（需要防止偏差与可被攻击）

工程建议：优先使用“可验证 reward”（规则/执行反馈），RM/LLM judge 用来覆盖不可完全形式化的质量维度（有用性、表达等）。

### 2) 偏好数据与 RM 训练要点

- 数据形式：\((x, y^+, y^-)\) 或排序列表
- 训练目标：让 RM 对偏好样本打更高分
- 风险：分布漂移、标注者偏差、提示注入与对抗样本

### 3) 指标体系：离线与在线要区分

离线常见：

- win-rate / preference rate（对比评测）
- 任务成功率（可执行任务）
- groundedness（是否引用/是否可追溯到证据）

在线常见：

- 用户满意度 proxy（但要防“讨好式对齐”）
- 失败率（拒答率、工具调用失败、格式错误）
- 安全事件率（越权、敏感输出）

### 4) 一个实用原则：把“质量”拆成可审计子项

例如把“好答案”拆成：

- 是否遵循指令（format / constraints）
- 是否引用证据（source / region / quote）
- 是否可执行/可验证（tests / validators）
- 是否安全（policy）

这样你的 reward 才不容易被 hack，评测也更可复现。

## 安全性与偏差控制

对齐不是“把模型变乖”，而是把模型行为约束到**可控、安全、可审计**的范围内。安全性与偏差控制需要同时发生在：数据、训练目标、推理策略、以及系统权限层。

### 1) 安全对齐的基本策略

- **数据层**：加入拒答/澄清/安全替代方案的高质量示例（SFT + 偏好数据）
- **目标层**：把安全约束显式纳入 reward（惩罚越权、惩罚无证据断言）
- **推理层**：模板与 guardrail（例如必须引用、必须结构化、必须走工具）
- **系统层**：权限与沙箱（能调用哪些工具、能访问哪些数据、关键动作二次确认）

### 2) 偏差与公平性：不要只靠“过滤数据”

偏差来源包括：

- 训练语料的历史偏差
- 偏好标注者的偏好与文化差异
- 评测集覆盖不足（忽略长尾用户）

工程上可做的事情：

- 对关键人群/场景做分桶评测（slice evaluation）
- 监控线上输出分布漂移（例如拒答率、敏感触发率）
- 建立“偏差事故”的回归集与持续评测

### 3) 安全上线清单（推荐最小集合）

- **红线策略**：明确哪些输出必须拒答/转人工
- **可审计日志**：请求、上下文、工具调用、最终输出与引用证据可回放
- **灰度与回滚**：对齐策略变更必须可控扩散
- **对抗测试**：提示注入、越权、数据泄漏、幻觉诱导

---

## 本章小结

- SFT 定义“助手形态”，RL 对齐定义“偏好与约束”，系统设计保证“可控与可审计”。
- PPO 的关键是“限制更新幅度 + KL 约束”；GRPO 用组内相对优势降低对 value baseline 的依赖。
- Reward 与评测体系决定上限：优先可验证 reward，质量拆成可审计子项，建立线上闭环与回归集。

